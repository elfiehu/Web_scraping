{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will build a web application that scrapes various websites for data related to the Mission to Mars and displays the information in a single HTML page. The following outlines what you need to do.\n",
    "\n",
    "## Step 1 - Scraping\n",
    "\n",
    "Complete your initial scraping using Jupyter Notebook, BeautifulSoup, Pandas, and Requests/Splinter.\n",
    "\n",
    "* Create a Jupyter Notebook file called `mission_to_mars.ipynb` and use this to complete all of your scraping and analysis tasks. The following outlines what you need to scrape.\n",
    "\n",
    "### NASA Mars News\n",
    "\n",
    "* Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and collect the latest News Title and Paragragh Text. Assign the text to variables that you can reference later.\n",
    "\n",
    "```python\n",
    "# Example:\n",
    "news_title = \"NASA's Next Mars Mission to Investigate Interior of Red Planet\"\n",
    "\n",
    "news_p = \"Preparation of NASA's next spacecraft to Mars, InSight, has ramped up this summer, on course for launch next May from Vandenberg Air Force Base in central California -- the first interplanetary launch in history from America's West Coast.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from os import getcwd\n",
    "from os.path import join\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_title = NASA Invests in Visionary Technology \n",
      "news_p = \n",
      "NASA's InSight spacecraft has arrived at Vandenberg Air Force Base in central California to begin final preparations for a launch this May.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# URL of nasa\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "html = requests.get(url)\n",
    "soup = bs(html.text, 'html.parser')\n",
    "results = soup.find_all('div', class_='image_and_description_container')\n",
    "content_title = results[0].find('img', class_=\"img-lazy\")['alt']\n",
    "content_title_1 = results[0].find('a')\n",
    "news_p = result.find('div', class_='rollover_description_inner').text\n",
    "print(\"content_title = \"+ content_title)\n",
    "print(\"news_p = \" + news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "### JPL Mars Space Images - Featured Image\n",
    "\n",
    "* Visit the url for JPL's Featured Space Image [here](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars).\n",
    "\n",
    "* Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called `featured_image_url`.\n",
    "\n",
    "* Make sure to find the image url to the full size `.jpg` image.\n",
    "\n",
    "* Make sure to save a complete url string for this image.\n",
    "\n",
    "```python\n",
    "# Example:\n",
    "featured_image_url = 'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA16225_hires.jpg'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-35150b1c1f21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'section'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'content_page module'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mfeatured_image_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'img'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'main_image'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatured_image_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "url = 'https://www.jpl.nasa.gov/spaceimages/details.php?id=PIA17563'\n",
    "browser.visit(url)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "results = soup.find_all('section', class_='content_page module')\n",
    "featured_image_url = result.find('img', class_='main_image')\n",
    "try:\n",
    "    print(featured_image_url)\n",
    "except:\n",
    "    print(\"no results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    html = soup.find_all('footer',class_='a href')\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    title = soup.find_all('span', class_='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Mars Weather\n",
    "\n",
    "* Visit the Mars Weather twitter account [here](https://twitter.com/marswxreport?lang=en) and scrape the latest Mars weather tweet from the page. Save the tweet text for the weather report as a variable called `mars_weather`.\n",
    "\n",
    "```python\n",
    "# Example:\n",
    "mars_weather = 'Sol 1801 (Aug 30, 2017), Sunny, high -21C/-5F, low -80C/-112F, pressure at 8.82 hPa, daylight 06:09-17:55'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TweepError",
     "evalue": "[{'code': 89, 'message': 'Invalid or expired token.'}]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b6004a012d76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mtarget_user\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"@MarsWxReport\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mall_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_timeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_user\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mmars_weather\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmars_weather\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\PythonData2\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;31m# Set pagination mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\PythonData2\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    227\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[1;31m# Parse the response payload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTweepError\u001b[0m: [{'code': 89, 'message': 'Invalid or expired token.'}]"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import json\n",
    "\n",
    "\n",
    "consumer_key = \"IM0MM9EIYEznGPI8dwzvRdJor\"\n",
    "consumer_secret = \"Eal8dLME44hmfVhRn04GZOJyAjPKbh1d8mUc20gyJIvVCDgIPD\"\n",
    "access_token = \"969400511928479744-CeLxAAwtNY55VPZCctXwx7ODuNcsLcG\"\n",
    "access_token_secret = \"jIprrB3mLX2Q64KqkxqOvi0Arvw9RR8ESWPU1UxlZWIZY\"\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "\n",
    "target_user = [\"@MarsWxReport\"]\n",
    "\n",
    "all_tweets = api.user_timeline(target_user)\n",
    "mars_weather = tweet[\"text\"][0]\n",
    "print(mars_weather)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Mars Facts\n",
    "\n",
    "* Visit the Mars Facts webpage [here](http://space-facts.com/mars/) and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "\n",
    "* Use Pandas to convert the data to a HTML table string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                      0                              1\n",
       " 0  Equatorial Diameter:                       6,792 km\n",
       " 1       Polar Diameter:                       6,752 km\n",
       " 2                 Mass:  6.42 x 10^23 kg (10.7% Earth)\n",
       " 3                Moons:            2 (Phobos & Deimos)\n",
       " 4       Orbit Distance:       227,943,824 km (1.52 AU)\n",
       " 5         Orbit Period:           687 days (1.9 years)\n",
       " 6  Surface Temperature:                  -153 to 20 °C\n",
       " 7         First Record:              2nd millennium BC\n",
       " 8          Recorded By:           Egyptian astronomers]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = 'https://space-facts.com/mars/'\n",
    "\n",
    "facts = ['Equatorial Diameter', 'Polar Diameter', 'Mass',\\\n",
    "         'Moons', 'Orbit Distance', 'Orbit Period'\\\n",
    "        'Surface Temperature', 'First Record', 'Recorded By']\n",
    "tables = pd.read_html(url)\n",
    "tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>facts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0                       ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               facts\n",
       "0                        0                       ..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars_df = pd.DataFrame(tables)\n",
    "\n",
    "mars_df.columns = ['facts']\n",
    "mars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame.to_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Mars Hemisperes\n",
    "\n",
    "* Visit the USGS Astrogeology site [here](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mar's hemispheres.\n",
    "\n",
    "* You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "\n",
    "* Save both the image url string for the full resolution hemipshere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys `img_url` and `title`.\n",
    "\n",
    "* Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\n",
    "\n",
    "\n",
    "```python\n",
    "# Example:\n",
    "hemisphere_image_urls = [\n",
    "    {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"...\"},\n",
    "    {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"...\"},\n",
    "    {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"...\"},\n",
    "    {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"...\"},\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg\n",
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg\n",
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg\n",
      "http://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg\n"
     ]
    }
   ],
   "source": [
    "from os import getcwd\n",
    "from os.path import join\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "#url = 'https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced'\n",
    "urls = ['https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced',\\\n",
    "        'https://astrogeology.usgs.gov/search/map/Mars/Viking/schiaparelli_enhanced',\\\n",
    "       'https://astrogeology.usgs.gov/search/map/Mars/Viking/syrtis_major_enhanced',\\\n",
    "       'https://astrogeology.usgs.gov/search/map/Mars/Viking/valles_marineris_enhanced']\n",
    "full_image_url = []\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.text, 'lxml')\n",
    "    result = soup.find('div', class_='downloads')\n",
    "    #print(result)\n",
    "    link = result.find('a')\n",
    "    full_image_url = link.get('href')\n",
    "    print(full_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3>Cerberus Hemisphere Enhanced</h3>\n",
      "<h3>Schiaparelli Hemisphere Enhanced</h3>\n",
      "<h3>Syrtis Major Hemisphere Enhanced</h3>\n",
      "<h3>Valles Marineris Hemisphere Enhanced</h3>\n"
     ]
    }
   ],
   "source": [
    "url_1 = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "response_1 = requests.get(url_1)\n",
    "soup = bs(response_1.text, 'lxml')\n",
    "results = soup.find_all('div', class_='collapsible results')\n",
    "#print(results)\n",
    "title = []\n",
    "for result in results:\n",
    "    titles = soup.find_all('h3')\n",
    "    for title in titles:\n",
    "        try:\n",
    "            print(title)\n",
    "        except:\n",
    "            print('something wrong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2 - MongoDB and Flask Application\n",
    "\n",
    "Use MongoDB with Flask templating to create a new HTML page that displays all of the information that was scraped from the URLs above.\n",
    "\n",
    "* Start by converting your Jupyter notebook into a Python script called `scrape_mars.py` with a function called `scrape` that will execute all of your scraping code from above and return one Python dictionary containing all of the scraped data.\n",
    "\n",
    "* Next, create a route called `/scrape` that will import your `scrape_mars.py` script and call your `scrape` function.\n",
    "\n",
    "  * Store the return value in Mongo as a Python dictionary.\n",
    "\n",
    "* Create a root route `/` that will query your Mongo database and pass the mars data into an HTML template to display the data.\n",
    "\n",
    "* Create a template HTML file called `index.html` that will take the mars data dictionary and display all of the data in the appropriate HTML elements. Use the following as a guide for what the final product should look like, but feel free to create your own design.\n",
    "\n",
    "![final_app_part1.png](Images/final_app_part1.png)\n",
    "![final_app_part2.png](Images/final_app_part2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
